{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11ef1731-c081-4c19-b1df-ea53b57cd4c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "      <td>4.526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "      <td>3.585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "      <td>3.521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20635</th>\n",
       "      <td>1.5603</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5.045455</td>\n",
       "      <td>1.133333</td>\n",
       "      <td>845.0</td>\n",
       "      <td>2.560606</td>\n",
       "      <td>39.48</td>\n",
       "      <td>-121.09</td>\n",
       "      <td>0.781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20636</th>\n",
       "      <td>2.5568</td>\n",
       "      <td>18.0</td>\n",
       "      <td>6.114035</td>\n",
       "      <td>1.315789</td>\n",
       "      <td>356.0</td>\n",
       "      <td>3.122807</td>\n",
       "      <td>39.49</td>\n",
       "      <td>-121.21</td>\n",
       "      <td>0.771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20637</th>\n",
       "      <td>1.7000</td>\n",
       "      <td>17.0</td>\n",
       "      <td>5.205543</td>\n",
       "      <td>1.120092</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>2.325635</td>\n",
       "      <td>39.43</td>\n",
       "      <td>-121.22</td>\n",
       "      <td>0.923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20638</th>\n",
       "      <td>1.8672</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5.329513</td>\n",
       "      <td>1.171920</td>\n",
       "      <td>741.0</td>\n",
       "      <td>2.123209</td>\n",
       "      <td>39.43</td>\n",
       "      <td>-121.32</td>\n",
       "      <td>0.847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20639</th>\n",
       "      <td>2.3886</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.254717</td>\n",
       "      <td>1.162264</td>\n",
       "      <td>1387.0</td>\n",
       "      <td>2.616981</td>\n",
       "      <td>39.37</td>\n",
       "      <td>-121.24</td>\n",
       "      <td>0.894</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20640 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0      8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1      8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2      7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3      5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4      3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "...       ...       ...       ...        ...         ...       ...       ...   \n",
       "20635  1.5603      25.0  5.045455   1.133333       845.0  2.560606     39.48   \n",
       "20636  2.5568      18.0  6.114035   1.315789       356.0  3.122807     39.49   \n",
       "20637  1.7000      17.0  5.205543   1.120092      1007.0  2.325635     39.43   \n",
       "20638  1.8672      18.0  5.329513   1.171920       741.0  2.123209     39.43   \n",
       "20639  2.3886      16.0  5.254717   1.162264      1387.0  2.616981     39.37   \n",
       "\n",
       "       Longitude  target  \n",
       "0        -122.23   4.526  \n",
       "1        -122.22   3.585  \n",
       "2        -122.24   3.521  \n",
       "3        -122.25   3.413  \n",
       "4        -122.25   3.422  \n",
       "...          ...     ...  \n",
       "20635    -121.09   0.781  \n",
       "20636    -121.21   0.771  \n",
       "20637    -121.22   0.923  \n",
       "20638    -121.32   0.847  \n",
       "20639    -121.24   0.894  \n",
       "\n",
       "[20640 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!pip install scikit-learn==1.3.0\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "import pandas as pd\n",
    "housing = fetch_california_housing(as_frame=True)\n",
    "df = pd.DataFrame(data=housing.data, columns=housing.feature_names)\n",
    "df['target'] = housing.target\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7001cd98-c133-4870-b614-113f8724f02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MedInc        0\n",
      "HouseAge      0\n",
      "AveRooms      0\n",
      "AveBedrms     0\n",
      "Population    0\n",
      "AveOccup      0\n",
      "Latitude      0\n",
      "Longitude     0\n",
      "target        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy='mean') # or 'median' or 'most_frequent'\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "df_dropped = df.dropna()\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "numerical_features = df.columns[:-1] # all columns except the target\n",
    "df_scaled = df.copy() # create a copy\n",
    "df_scaled[numerical_features] = scaler.fit_transform(df[numerical_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb23997c-e0e3-443b-a11d-c043045f8518",
   "metadata": {},
   "outputs": [],
   "source": [
    "Preprocessing steps and their justifications for the California Housing dataset:\n",
    "\n",
    "1. Handling Missing Values:\n",
    "The first step involves checking for and handling missing values within the dataset. Used df.isnull().sum() to identify the existence and extent of missing data in each column. If missing values had been present, techniques such as imputation (filling in missing values with estimates like the mean or median) using SimpleImputer or removing rows/columns with missing values using dropna() is used.\n",
    "Reasoning: Missing values can create issues during model training, leading to biased or inaccurate results. Handling them ensures data completeness and integrity for better model performance and reliability. In case, the dataset might contain missing values in real-world scenarios, this step would be crucial.\n",
    "\n",
    "2. Feature Scaling (Standardization):\n",
    "The second step involves scaling numerical features using standardization. We applied the StandardScaler to transform the data by subtracting the mean and dividing by the standard deviation for each feature.\n",
    "Reasoning: The California Housing dataset likely has features with varying scales (e.g., population, median income). Standardization brings all features to a similar scale (mean of 0 and standard deviation of 1), preventing features with larger values from disproportionately influencing the model. This improves the performance and stability of algorithms sensitive to feature scales, such as linear regression, k-nearest neighbors, and support vector machines.\n",
    "In conclusion, these preprocessing steps are essential for preparing the California Housing dataset for machine learning tasks:\n",
    "\n",
    "Handling missing values ensures data completeness and prevents biases or errors in the analysis.\n",
    "Feature scaling (standardization) levels the playing field for features, optimizing the performance of various machine learning algorithms.\n",
    "By performing these steps, the aim is to enhance the quality of the data and improve the accuracy and reliability of any subsequent analysis or model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ce3abc5-5a1d-46f4-93c7-a8176e749b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.5558915986952442\n",
      "R-squared: 0.575787706032451\n"
     ]
    }
   ],
   "source": [
    "#Step 1: Import necessary libraries\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "#Step 2: Split data into training and testing sets\n",
    "X = df_scaled.drop('target', axis=1)  # Features\n",
    "y = df_scaled['target']  # Target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # 80% train, 20% test\n",
    "\n",
    "#Step 3: Create and train the Linear Regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#Step 4: Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "#Step 5: Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7433ee1-ddfe-4adc-9ec0-67881d93ddd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Code explained:\n",
    "\n",
    "Import Libraries: import LinearRegression, train_test_split, mean_squared_error, and r2_score for model creation, data splitting, and evaluation.\n",
    "Split Data: the dataset is divided into training and testing sets using train_test_split. This is crucial to assess how well the model generalizes to unseen data.\n",
    "Create and Train: Create a LinearRegression object and train it using the training data (X_train, y_train). The model learns the relationships between features and the target variable.\n",
    "Make Predictions: Use the trained model to predict target values for the test set (X_test).\n",
    "Evaluate Model: Evaluate the model's performance using metrics like Mean Squared Error (MSE) and R-squared. These metrics provide insights into the model's accuracy and goodness of fit.\n",
    "\n",
    "Reasoning why Linear Regression might be suitable for the California Housing dataset:\n",
    "\n",
    "1. Linear Relationship: Linear Regression assumes a linear relationship between the features and the target variable. In the California Housing dataset, we expect features like median income, housing median age, and average rooms to have some degree of linear correlation with the median house value (target variable). This makes Linear Regression a potentially appropriate model for this dataset.\n",
    "\n",
    "2. Interpretability: Linear Regression offers excellent interpretability. The coefficients associated with each feature provide insights into the direction and magnitude of their impact on the target variable. This is valuable for understanding the factors influencing housing prices.\n",
    "\n",
    "3. Simplicity and Efficiency: Linear Regression is relatively simple to implement and computationally efficient, especially for datasets of moderate size like the California Housing dataset. This makes it a practical choice for initial exploration and analysis.\n",
    "\n",
    "4. Baseline Model: Linear Regression often serves as a good baseline model. Even if it might not be the most accurate model, it can provide a benchmark against which to compare more complex models.\n",
    "\n",
    "5. Data Characteristics: The California Housing dataset is relatively clean and well-structured. It has numerical features and a continuous target variable, which align with the requirements of Linear Regression.\n",
    "\n",
    "However, this method assumes linearity, which might not perfectly capture all complexities in real-world housing data.\n",
    "Outliers can significantly impact the model, so it's crucial to handle them appropriately.\n",
    "Feature scaling (standardization) is often necessary to improve performance, as was done in the preprocessing steps.\n",
    "Despite these considerations, Linear Regression seems a suitable starting point for modeling the California Housing dataset due to its simplicity, interpretability, potential for capturing linear relationships, and appropriateness for the dataset's characteristics. It's a valuable tool for initial exploration and provides a baseline for comparing more complex models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "862b9764-cb82-4270-9bc2-157281f4cdf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.4942716777366763\n",
      "R-squared: 0.6228111330554302\n"
     ]
    }
   ],
   "source": [
    "#Step 1: Import necessary libraries\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "#Step 2: Split data into training and testing sets\n",
    "X = df_scaled.drop('target', axis=1)  # Features\n",
    "y = df_scaled['target']  # Target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # 80% train, 20% test\n",
    "\n",
    "#Step 3: Create and train the Decision Tree Regressor model\n",
    "model = DecisionTreeRegressor(random_state=42)  #hyperparameters can be adjusted here\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#Step 4: Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "#Step 5: Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4941c36c-5c71-4f16-bc9a-9d3f8b4d44b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Code explained:\n",
    "\n",
    "Import Libraries: import DecisionTreeRegressor, train_test_split, mean_squared_error, and r2_score for model creation, data splitting, and evaluation.\n",
    "Split Data: the dataset is split into training and testing sets using train_test_split to assess the model's generalization ability.\n",
    "Create and Train: Created a DecisionTreeRegressor object and train it on the training data. Hyperparameters like max_depth, min_samples_split, and min_samples_leaf can be tuned to optimize performance.\n",
    "Make Predictions: Use the trained model to predict target values for the test set.\n",
    "Evaluate Model: Evaluate the model using metrics like Mean Squared Error (MSE) and R-squared to measure its accuracy and goodness of fit.\n",
    "\n",
    "Reasoning why a Decision Tree Regressor might be suitable for the California Housing dataset:\n",
    "\n",
    "1. Handling Non-linear Relationships: Unlike Linear Regression, Decision Trees can capture non-linear relationships between features and the target variable. This is beneficial for housing data, where factors like location and proximity to amenities might have non-linear impacts on prices.\n",
    "\n",
    "2. Feature Interactions: Decision Trees automatically consider interactions between features when making predictions. This is important in housing data, where the combined effect of features (e.g., income and school quality) might be more significant than their individual effects.\n",
    "\n",
    "3. Interpretability: Decision Trees are relatively easy to interpret. The tree structure visually represents the decision-making process, allowing us to understand how features contribute to predictions.\n",
    "\n",
    "4. Handling Outliers: Decision Trees are less sensitive to outliers compared to Linear Regression. They partition data based on feature thresholds, making them more robust to extreme values.\n",
    "\n",
    "5. No Feature Scaling: Decision Trees generally don't require feature scaling (standardization). This simplifies the preprocessing steps and can be advantageous in certain cases.\n",
    "\n",
    "However, Decision Trees can be prone to overfitting, especially with complex trees. Techniques like pruning or setting hyperparameter limits are often used to address this.\n",
    "Small changes in the data can lead to significant changes in the tree structure, potentially affecting stability.\n",
    "Interpretability can decrease with increasing tree complexity.\n",
    "But overall, Decision Tree Regressors are potentially suitable for the California Housing dataset because of their ability to handle non-linear relationships, feature interactions, robustness to outliers, and relative ease of interpretation. While overfitting and instability are potential concerns, techniques like pruning and hyperparameter tuning can mitigate these issues. The choice between Linear Regression and Decision Tree Regressor often depends on the specific characteristics of the dataset and the desired balance between interpretability and predictive accuracy. In some cases, ensemble methods (like Random Forests) that combine multiple decision trees might offer even better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d1a0b46-890b-46c5-adf0-fd87402c724c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.25549776668540763\n",
      "R-squared: 0.805024407701793\n"
     ]
    }
   ],
   "source": [
    "#Step 1: Import necessary libraries\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "#Step 2: Split data into training and testing sets\n",
    "X = df_scaled.drop('target', axis=1)  # Features\n",
    "y = df_scaled['target']  # Target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # 80% train, 20% test\n",
    "\n",
    "#Step 3: Create and train the Random Forest Regressor model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#Step 4: Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "#Step 5: Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93db0534-5017-4907-bf5a-d27d0c5ca85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Code explained:\n",
    "\n",
    "Import Libraries: import RandomForestRegressor, train_test_split, mean_squared_error, and r2_score for model creation, data splitting, and evaluation.\n",
    "Split Data: split the dataset into training and testing sets using train_test_split to assess the model's generalization ability on unseen data.\n",
    "Create and Train: create a RandomForestRegressor object and train it on the training data. Hyperparameters like n_estimators, max_depth, min_samples_split, and min_samples_leaf can be tuned to optimize performance.\n",
    "Make Predictions: use the trained model to predict target values for the test set.\n",
    "Evaluate Model: evaluate the model using metrics like Mean Squared Error (MSE) and R-squared to measure its accuracy and goodness of fit.\n",
    "\n",
    "Reasoning why a Random Forest Regressor might be suitable for the California Housing dataset:\n",
    "\n",
    "1. Handling Non-linearity and Interactions: Like Decision Trees, Random Forests excel at capturing non-linear relationships and interactions between features. This is crucial for housing data, where factors like location, proximity to amenities, and neighborhood characteristics can have complex effects on prices.\n",
    "\n",
    "2. Reduced Overfitting: Random Forests mitigate overfitting, a common issue with Decision Trees, by averaging predictions from multiple trees. This ensemble approach improves generalization and makes the model more robust to noise in the data.\n",
    "\n",
    "3. Feature Importance: Random Forests provide valuable insights into feature importance, helping us understand which factors are most influential in predicting housing prices. This can be useful for feature selection and gaining domain knowledge.\n",
    "\n",
    "4. Robustness to Outliers: Random Forests are less sensitive to outliers compared to Linear Regression, as they rely on the collective wisdom of multiple trees.\n",
    "\n",
    "5. Handling Missing Values (Imputation): Some implementations of Random Forests can handle missing values directly without requiring imputation beforehand. This can simplify the preprocessing steps.\n",
    "\n",
    "However, Random Forests can be computationally more intensive than Linear Regression or single Decision Trees, especially with large datasets or many trees.\n",
    "Interpretability can be slightly reduced compared to single Decision Trees, as the model combines predictions from multiple trees. Overall, Random Forest Regressors are often a strong choice for regression tasks, including the California Housing dataset, due to their ability to handle non-linearity, interactions, reduce overfitting, provide feature importance, and offer robustness to outliers. While they might be computationally more demanding, their potential for improved accuracy and insights often outweighs this consideration.\n",
    "In comparison to Linear Regression and Decision Trees:\n",
    "Linear Regression: Might be too simplistic for capturing complex relationships in housing data.\n",
    "Decision Tree: Prone to overfitting, while Random Forests address this issue through ensembling.\n",
    "Therefore, Random Forest Regressor is often considered a more suitable and robust option for the California Housing dataset, especially when aiming for higher predictive accuracy and handling potential non-linearities and interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d9db991-5680-44c2-812e-bb2f9049cd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.29399901242474274\n",
      "R-squared: 0.7756433164710084\n"
     ]
    }
   ],
   "source": [
    "#Step 1: Import necessary libraries\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "#Step 2: Split data into training and testing sets\n",
    "X = df_scaled.drop('target', axis=1)  # Features\n",
    "y = df_scaled['target']  # Target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # 80% train, 20% test\n",
    "\n",
    "#Step 3: Create and train the Gradient Boosting Regressor model\n",
    "model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#Step 4: Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "#Step 5: Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c3ee66-454d-49e5-bad3-a4c34a33adcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Code explained:\n",
    "\n",
    "Import Libraries: import GradientBoostingRegressor, train_test_split, mean_squared_error, and r2_score for model creation, data splitting, and evaluation.\n",
    "Split Data: split the dataset into training and testing sets using train_test_split to assess the model's performance on unseen data.\n",
    "Create and Train: create a GradientBoostingRegressor object and train it on the training data. Hyperparameters like n_estimators, learning_rate, max_depth, and min_samples_split can be tuned to optimize performance.\n",
    "Make Predictions: use the trained model to predict target values for the test set.\n",
    "Evaluate Model: evaluate the model using metrics like Mean Squared Error (MSE) and R-squared to measure its accuracy and goodness of fit.\n",
    "\n",
    "Reasoning why a Gradient Boosting Regressor might be suitable for the California Housing dataset:\n",
    "\n",
    "1. Handling Non-linearity and Interactions: Similar to Random Forests, Gradient Boosting excels at capturing non-linear relationships and interactions between features. This is crucial for housing data, where factors like location, proximity to amenities, and neighborhood characteristics can have complex, interwoven effects on prices.\n",
    "\n",
    "2. High Predictive Accuracy: Gradient Boosting is known for its high predictive accuracy. It often outperforms other regression algorithms, including Linear Regression, Decision Trees, and even Random Forests in many cases. This makes it a strong contender for the California Housing dataset, where accurate predictions are desirable.\n",
    "\n",
    "3. Sequential Improvement: Gradient Boosting's sequential learning process allows it to iteratively improve predictions by focusing on errors made by previous trees. This leads to a more refined and accurate model.\n",
    "\n",
    "4. Regularization: Gradient Boosting incorporates regularization techniques, such as shrinkage (learning rate) and subsampling, to prevent overfitting and improve generalization to unseen data.\n",
    "\n",
    "5. Feature Importance: Like Random Forests, Gradient Boosting provides insights into feature importance, helping us understand which factors are most influential in predicting housing prices. This can be valuable for feature selection and gaining domain knowledge.\n",
    "\n",
    "However, Gradient Boosting can be computationally more intensive than Linear Regression or single Decision Trees, especially with large datasets or many trees. Hyperparameter tuning might also require more computational resources.\n",
    "Interpretability can be slightly reduced compared to single Decision Trees, as the model combines predictions from multiple trees in a complex way.\n",
    "Overall, Gradient Boosting Regressors are often a top choice for regression tasks, including the California Housing dataset, due to their potential for high predictive accuracy, ability to handle non-linearity and interactions, sequential improvement, regularization, and feature importance. While they might be computationally more demanding, their potential for superior performance often outweighs this consideration.\n",
    "\n",
    "In comparison to other algorithms:                                                                                                                        \n",
    "Linear Regression: Too simplistic for capturing complex relationships in housing data.\n",
    "Decision Tree: Prone to overfitting, which Gradient Boosting addresses through its ensemble approach and regularization.\n",
    "Random Forest: While generally robust, Gradient Boosting often achieves even higher accuracy due to its sequential learning process.\n",
    "Therefore, Gradient Boosting Regressor is often considered a highly suitable and powerful option for the California Housing dataset, especially when aiming for top-notch predictive performance and handling potential non-linearities and interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d55b00f-dd56-45a1-b14b-70cdc434f29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.3551984619989419\n",
      "R-squared: 0.7289407597956462\n"
     ]
    }
   ],
   "source": [
    "#Step 1: Import necessary libraries\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "#Step 2: Split data into training and testing sets\n",
    "X = df_scaled.drop('target', axis=1)  # Features\n",
    "y = df_scaled['target']  # Target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # 80% train, 20% test\n",
    "\n",
    "#Step 3: Create and train the SVR model\n",
    "model = SVR(kernel='rbf')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#Step 4: Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "#Step 5: Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3e3423-02f9-4f2e-aba5-02b25ff94de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Code explained:\n",
    "\n",
    "Import Libraries: import SVR, train_test_split, mean_squared_error, and r2_score for model creation, data splitting, and evaluation.\n",
    "Split Data: split the dataset into training and testing sets using train_test_split to assess the model's performance on unseen data.\n",
    "Create and Train: create an SVR object and train it on the training data. Hyperparameters like kernel (e.g., 'linear', 'rbf', 'poly'), C (regularization parameter), and epsilon (tolerance for errors) can be tuned to optimize performance.\n",
    "Make Predictions: use the trained model to predict target values for the test set.\n",
    "Evaluate Model: evaluate the model using metrics like Mean Squared Error (MSE) and R-squared to measure its accuracy and goodness of fit.\n",
    "\n",
    "Reasoning why a Support Vector Regressor (SVR) might be suitable for the dataset:\n",
    "\n",
    "1. Handling Non-linearity: SVR, particularly with non-linear kernels like the Radial Basis Function (RBF) kernel, can effectively capture non-linear relationships between features and the target variable. This is crucial for housing data, where factors like location and proximity to amenities might have non-linear impacts on prices.\n",
    "\n",
    "2. Regularization: SVR incorporates regularization through the C parameter, which helps prevent overfitting and improves the model's generalization ability. This is important for ensuring that the model performs well on unseen data.\n",
    "\n",
    "3. Handling Outliers: SVR is relatively robust to outliers due to its focus on maximizing the margin between support vectors. This makes it less sensitive to extreme values in the data compared to some other regression algorithms.\n",
    "\n",
    "4. Versatility: SVR offers flexibility through different kernel choices (e.g., linear, polynomial, RBF). This allows you to adapt the model to various data patterns and complexities.\n",
    "\n",
    "5. Feature Scaling: While not strictly required, feature scaling (standardization) often improves the performance of SVR, especially with kernels like RBF. This is because SVR relies on distances between data points, and scaling ensures that features with larger values don't disproportionately influence the model.\n",
    "\n",
    "However, SVR can be computationally more intensive than Linear Regression or Decision Trees, especially with large datasets. Hyperparameter tuning can also require more computational resources.\n",
    "Interpretability can be more challenging compared to Linear Regression or Decision Trees, as the model's decision boundaries are defined by support vectors and kernel functions.\n",
    "Overall, SVR is a potentially suitable option for the California Housing dataset due to its ability to handle non-linearity, incorporate regularization, handle outliers, offer versatility, and benefit from feature scaling. While it might be computationally more demanding and less interpretable than some other algorithms, its potential for capturing complex relationships and achieving good predictive performance makes it a valuable consideration.\n",
    "\n",
    "In comparison to other algorithms:\n",
    "Linear Regression: Might be too simplistic for capturing complex relationships in housing data.\n",
    "Decision Tree: Prone to overfitting, while SVR addresses this through regularization.\n",
    "Random Forest/Gradient Boosting: Often achieve high accuracy, but SVR might be preferred in cases where non-linearity is prominent or when a more robust approach to outliers is desired.\n",
    "Therefore, SVR is worth exploring for the California Housing dataset, especially when aiming for a balance between handling non-linearity, preventing overfitting, and achieving good predictive accuracy. The choice between SVR and other algorithms ultimately depends on the specific characteristics of the data and the desired trade-offs between accuracy, interpretability, and computational cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ccb4f81a-e3b8-4a47-a4bb-1b7691034c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression: MSE = 0.5559\n",
      "Decision Tree: MSE = 0.4943\n",
      "Random Forest: MSE = 0.2555\n",
      "Gradient Boosting: MSE = 0.2940\n",
      "SVR: MSE = 0.3552\n"
     ]
    }
   ],
   "source": [
    "#Evaluating the performance of each algorithm using the Mean Squared Error (MSE)\n",
    "\n",
    "#Step 1: Import necessary libraries\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#Step 2: Split data into training and testing sets\n",
    "X = df_scaled.drop('target', axis=1)  #Features\n",
    "y = df_scaled['target']  #Target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Step 3: Train and evaluate each algorithm\n",
    "#Train and Evaluate: iterate through each algorithm, train it on the training data, make predictions on the test data, and calculate the MSE. The results are stored in a dictionary.\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(random_state=42),\n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42),\n",
    "    \"SVR\": SVR(kernel='rbf'),\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    results[name] = mse\n",
    "\n",
    "#Step 4: Print the results\n",
    "for name, mse in results.items():\n",
    "    print(f\"{name}: MSE = {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c96b993c-7224-41e2-8ca5-4d144c2ced7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression: MAE = 0.5332\n",
      "Decision Tree: MAE = 0.4538\n",
      "Random Forest: MAE = 0.3276\n",
      "Gradient Boosting: MAE = 0.3717\n",
      "SVR: MAE = 0.3978\n"
     ]
    }
   ],
   "source": [
    "#Evaluating the performance of each algorithm using the Mean Absolute Error (MAE)\n",
    "\n",
    "#Step 1: Import necessary libraries\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error  # Import MAE\n",
    "\n",
    "#Step 2: Split data into training and testing sets\n",
    "X = df_scaled.drop('target', axis=1)\n",
    "y = df_scaled['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Step 3: Train and evaluate each algorithm\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(random_state=42),\n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42),\n",
    "    \"SVR\": SVR(kernel='rbf'),\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, y_pred)  #Calculate MAE\n",
    "    results[name] = mae\n",
    "\n",
    "for name, mae in results.items():\n",
    "    print(f\"{name}: MAE = {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eaa9b17-00b3-40dd-98d5-530d1f89824f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The algorithm with the lowest MAE generally indicates better predictive accuracy in terms of the average absolute difference between predicted and actual values.\n",
    "#Further tuning and cross-validation might be needed for a more robust evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b35e4f80-6b36-47fa-995c-00f116520798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression: RÂ² = 0.5758\n",
      "Decision Tree: RÂ² = 0.6228\n",
      "Random Forest: RÂ² = 0.8050\n",
      "Gradient Boosting: RÂ² = 0.7756\n",
      "SVR: RÂ² = 0.7289\n"
     ]
    }
   ],
   "source": [
    "#Evaluating the performance of each algorithm using R-squared Score (RÂ²)\n",
    "\n",
    "#Step 1: Import necessary libraries\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score  # Import R-squared\n",
    "\n",
    "#Step 2: Split data into training and testing sets\n",
    "X = df_scaled.drop('target', axis=1)\n",
    "y = df_scaled['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Step 3: Train and evaluate each algorithm\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(random_state=42),\n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42),\n",
    "    \"SVR\": SVR(kernel='rbf'),\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    r2 = r2_score(y_test, y_pred)  # Calculate R-squared\n",
    "    results[name] = r2\n",
    "\n",
    "for name, r2 in results.items():\n",
    "    print(f\"{name}: RÂ² = {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87675309-b57c-4ae5-a98f-26b738bcad3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The algorithm with the highest R-squared score generally indicates better goodness of fit, \n",
    "# meaning it explains a larger proportion of the variance in the target variable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
